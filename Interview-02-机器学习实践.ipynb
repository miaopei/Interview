{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 超参数选择\n",
    "\n",
    "## Grid Search\n",
    "\n",
    "- 网格搜索\n",
    "- 在高维空间中对一定区域进行遍历\n",
    "\n",
    "## Random Search\n",
    "\n",
    "- 在高维空间中随机选择若干超参数\n",
    "\n",
    "## 相关库（未使用）\n",
    "\n",
    "- [Hyperopt](http://hyperopt.github.io/hyperopt/)\n",
    "  - 用于超参数优化的 Python 库，其内部使用 Parzen 估计器的树来预测哪组超参数可能会得到好的结果。\n",
    "  - GitHub - https://github.com/hyperopt/hyperopt\n",
    "  \n",
    "- [Hyperas](http://maxpumperla.com/hyperas/)\n",
    "  - 将 Hyperopt 与 Keras 模型集成在一起的库\n",
    "  - GitHub - https://github.com/maxpumperla/hyperas\n",
    "  \n",
    "## Reference\n",
    "\n",
    "- []()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 几种参数估计的区别于联系: MLE、MAP、贝叶斯\n",
    "\n",
    "\n",
    "## Reference\n",
    "\n",
    "- []()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 余弦相似度（Cos距离）与欧氏距离的区别和联系\n",
    "\n",
    "> geekcircle/machine-learning-interview-qa/[4.md](https://github.com/geekcircle/machine-learning-interview-qa/blob/master/questions/4.md)\n",
    "\n",
    "- 欧式距离和余弦相似度都能度量 2 个向量之间的相似度\n",
    "- 放到向量空间中看，欧式距离衡量两点之间的**直线距离**，而余弦相似度计算的是两个向量之间的**夹角**\n",
    "- **没有归一化时**，欧式距离的范围是 $(0, +∞]$，而余弦相似度的范围是 $(0, 1]$；余弦距离是计算**相似程度**，而欧氏距离计算的是**相同程度**（对应值的相同程度）\n",
    "- **归一化的情况下**，可以将空间想象成一个超球面（三维），欧氏距离就是球面上两点的直线距离，而向量余弦值等价于两点的球面距离，本质是一样。\n",
    "\n",
    "> [欧氏距离和余弦相似度的区别是什么？]() - 知乎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 数据标准化 / 归一化 normalization\n",
    "\n",
    "\n",
    "## Reference\n",
    "\n",
    "- [数据标准化/归一化normalization](https://blog.csdn.net/pipisorry/article/details/52247379)\n",
    "\n",
    "- [机器学习中常见的几种归一化方法以及原因](https://blog.csdn.net/UESTC_C2_403/article/details/75804617)\n",
    "\n",
    "- [标准化和归一化什么区别？](https://www.zhihu.com/question/20467170)\n",
    "\n",
    "- [L2范数归一化](https://blog.csdn.net/geekmanong/article/details/51344732)\n",
    "\n",
    "- [归一化和标准化的一些理解](https://www.jianshu.com/p/540d56ef350f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 监督学习和无监督学习\n",
    "\n",
    "> geekcircle/machine-learning-interview-qa/[6.md](https://github.com/geekcircle/machine-learning-interview-qa/blob/master/questions/6.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 熵，求投掷均匀正六面体骰子的熵\n",
    "\n",
    "> geekcircle/machine-learning-interview-qa/[7.md](https://github.com/geekcircle/machine-learning-interview-qa/blob/master/questions/7.md)\n",
    "\n",
    "什么是熵？\n",
    "\n",
    "- 熵是为消除不确定性所需要获得的信息量\n",
    "\n",
    "## 信息熵、KL 散度（相对熵）与交叉熵\n",
    "\n",
    "> 《深度学习》 3.13 信息论\n",
    "\n",
    "信息论的基本想法是一个不太可能的事件居然发生了，要比一个非常可能的事件发生，能提供更多的信息。\n",
    "\n",
    "该想法可描述为以下性质：\n",
    "\n",
    "1. 非常可能发生的事件信息量要比较少，并且极端情况下，确保能够发生的事件应该没有信息量。\n",
    "2. 比较不可能发生的事件具有更高的信息量。\n",
    "3. 独立事件应具有增量的信息。例如，投掷的硬币两次正面朝上传递的信息量，应该是投掷一次硬币正面朝上的信息量的两倍。\n",
    "\n",
    "## 自信息与信息熵\n",
    "\n",
    "自信息（self-information）是一种量化以上性质的函数，定义一个事件 x 的自信息为：\n",
    "\n",
    "<div align=\"center\"><img src=\"http://latex.codecogs.com/gif.latex?I(x) = - \\log P(x)\" /></div>\n",
    "\n",
    "> 当该对数的底数为 e 时，单位为奈特（nats，本书标准）；当以 2 为底数时，单位为比特（bit）或香农（shannons）\n",
    "\n",
    "自信息只处理单个的输出。此时，用信息熵（Information-entropy）来对整个概率分布中的不确定性总量进行量化：\n",
    "\n",
    "<div align=\"center\"><img src=\"http://latex.codecogs.com/gif.latex?H(\\mathrm{X}) = \\mathbb{E}_{\\mathrm{X} \\sim P}[I(x)] = - \\sum_{x \\in \\mathrm{X}}P(x) \\log P(x)\" /></div>\n",
    "\n",
    "> 信息熵也称香农熵（Shannon entropy）\n",
    ">\n",
    "> 信息论中，记 `0log0 = 0`\n",
    "\n",
    "## 相对熵（KL 散度）与交叉熵\n",
    "\n",
    "P 对 Q 的 **KL散度**（Kullback-Leibler divergence）：\n",
    "\n",
    "<div align=\"center\"><img src=\"http://latex.codecogs.com/gif.latex?D_P(Q) = \\mathbb{E}_{\\mathrm{X} \\sim P} \\left[ \\log \\frac{P(x)}{Q(x)} \\right] = \\sum_{x \\in \\mathrm{X}}P(x) \\left[ \\log P(x) - \\log Q(x) \\right]\" /></div>\n",
    "\n",
    "**KL 散度在信息论中度量的是那个直观量**：\n",
    "\n",
    "在离散型变量的情况下， KL 散度衡量的是，当我们使用一种被设计成能够使得概率分布 Q 产生的消息的长度最小的编码，发送包含由概率分布 P 产生的符号的消息时，所需要的额外信息量。\n",
    "\n",
    "**KL 散度的性质**：\n",
    "- 非负；KL 散度为 0 当且仅当P 和 Q 在离散型变量的情况下是相同的分布，或者在连续型变量的情况下是“几乎处处”相同的\n",
    "- 不对称；$D_p(q) != D_q(p)$\n",
    "\n",
    "**交叉熵**（cross-entropy）：\n",
    "\n",
    "<div align=\"center\"><img src=\"http://latex.codecogs.com/gif.latex?H_P(Q) = - \\mathbb{E}_{\\mathrm{X} \\sim  P} \\log Q(x) = - \\sum_{x \\in \\mathrm{X}}P(x) \\log Q(x)\" /></div>\n",
    "\n",
    "> [信息量，信息熵，交叉熵，KL散度和互信息（信息增益）](https://blog.csdn.net/haolexiao/article/details/70142571) - CSDN博客\n",
    "\n",
    "**交叉熵与 KL 散度的关系**：\n",
    "\n",
    "<div align=\"center\"><img src=\"http://latex.codecogs.com/gif.latex?H_P(Q) = H(P) + D_P(Q)\" /></div>\n",
    "\n",
    "**针对 Q 最小化交叉熵等价于最小化 P 对 Q 的 KL 散度**，因为 Q 并不参与被省略的那一项。\n",
    "\n",
    "最大似然估计中，最小化 KL 散度其实就是在最小化分布之间的交叉熵。\n",
    "\n",
    "> 《深度学习》 ch5.5 - 最大似然估计\n",
    "\n",
    "**求投掷均匀正六面体骰子的熵**\n",
    "\n",
    "- 问题描述：向空中投掷硬币，落地后有两种可能的状态，一个是正面朝上，另一个是反面朝上，每个状态出现的概率为1/2。如投掷均匀的正六面体的骰子，则可能会出现的状态有6个，每一个状态出现的概率均为1/6。试通过计算来比较状态的不确定性与硬币状态的不确定性的大小。\n",
    "\n",
    "- 答：\n",
    "\n",
    "    硬币：<div align=\"center\"><img src=\"http://latex.codecogs.com/gif.latex?- \\sum^{n}_{i=1}P(X_i) \\log P(X_i) = -2 * \\frac{1}{2} * \\log P(\\frac{1}{2}) \\approx 1 \\text{bit}\" /></div>\n",
    "\n",
    "    六面体：<div align=\"center\"><img src=\"http://latex.codecogs.com/gif.latex?- \\sum^{n}_{i=1}P(X_i) \\log P(X_i) = -6 * \\frac{1}{6} * \\log P(\\frac{1}{6}) \\approx 2.6 \\text{bit}\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 混淆矩阵、模型度量指标：准确率、精确率、召回率、F1 值等\n",
    "\n",
    "**混淆矩阵**\n",
    "\n",
    "- True Positive(TP)：将正类预测为正类的数量.\n",
    "- True Negative(TN)：将负类预测为负类的数量.\n",
    "- False Positive(FP)：将负类预测为正类数 → 误报 (Type I error).\n",
    "- False Negative(FN)：将正类预测为负类数 → 漏报 (Type II error).\n",
    "\n",
    "    <div align=\"center\"><img src=\"source/confusion_matrix.png\" height=\"\" /></div>\n",
    "\n",
    "**准确率**（accuracy）\n",
    "\n",
    "<div align=\"center\"><img src=\"http://latex.codecogs.com/gif.latex?ACC = \\frac{TP + TN}{TP + TN + FP + FN}\" /></div>\n",
    "\n",
    "**精确率**（precision）\n",
    "\n",
    "<div align=\"center\"><img src=\"http://latex.codecogs.com/gif.latex?P = \\frac{TP}{TP + FP}\" /></div>\n",
    "\n",
    "> 准确率与精确率的区别：\n",
    ">> 在正负样本不平衡的情况下，**准确率**这个评价指标有很大的缺陷。比如在互联网广告里面，点击的数量是很少的，一般只有千分之几，如果用acc，即使全部预测成负类（不点击）acc 也有 99% 以上，没有意义。\n",
    "    \n",
    "**召回率**（recall, sensitivity, true positive rate）\n",
    "\n",
    "<div align=\"center\"><img src=\"http://latex.codecogs.com/gif.latex?R = \\frac{TP}{TP + FN}\" /></div>\n",
    "\n",
    "**F1值**——精确率和召回率的调和均值\n",
    "\n",
    "<div align=\"center\"><img src=\"http://latex.codecogs.com/gif.latex?\\frac{2}{F_{1}} = \\frac{1}{P} + \\frac{1}{R}\" /></div>\n",
    "\n",
    "<div align=\"center\"><img src=\"http://latex.codecogs.com/gif.latex?F_{1} = \\frac{2TP}{2TP + FP + FN}\" /></div>\n",
    "\n",
    "> 只有当精确率和召回率都很高时，F1值才会高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 如何处理数据中的缺失值\n",
    "\n",
    "> geekcircle/machine-learning-interview-qa/[1.md](https://github.com/geekcircle/machine-learning-interview-qa/blob/master/questions/1.md)\n",
    "\n",
    "可以分为以下 2 种情况：\n",
    "\n",
    "1. **缺失值较多**\n",
    "\n",
    "    - 直接舍弃该列特征，否则可能会带来较大的噪声，从而对结果造成不良影响。\n",
    "    \n",
    "1. **缺失值较少**\n",
    "\n",
    "    - 当缺失值较少（<10%）时，可以考虑对缺失值进行填充，以下是几种常用的填充策略：\n",
    "    \n",
    "    i. 用一个**异常值**填充（比如 0），将缺失值作为一个特征处理\n",
    "\n",
    "        ` data.fillna(0) `\n",
    "\n",
    "    ii. 用**均值** | **条件均值**填充\n",
    "    \n",
    "        > 如果数据是不平衡的，那么应该使用条件均值填充\n",
    "        >\n",
    "        > 所谓**条件均值**，指的是与缺失值所属标签相同的所有数据的均值\n",
    "\n",
    "        `data.fillna(data.mean())`\n",
    "\n",
    "    iii. 用相邻数据填充\n",
    "\n",
    "        ```\n",
    "        # 用前一个数据填充\n",
    "        data.fillna(method='pad')\n",
    "        # 用后一个数据填充\n",
    "        data.fillna(method='bfill') \n",
    "        ```\n",
    "        \n",
    "    iv. 插值\n",
    "\n",
    "        `data.interpolate()`\n",
    "\n",
    "    v. 拟合\n",
    "    \n",
    "        > 简单来说，就是将缺失值也作为一个预测问题来处理：将数据分为正常数据和缺失数据，对有值的数据采用随机森林等方法拟合，然后对有缺失值的数据进行预测，用预测的值来填充。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. 介绍一个完整的机器学习项目流程\n",
    "\n",
    "> geekcircle/machine-learning-interview-qa/[2.md](https://github.com/geekcircle/machine-learning-interview-qa/blob/master/questions/2.md)\n",
    "\n",
    "### 1. 数学抽象\n",
    "\n",
    "- 明确问题是进行机器学习的第一步。机器学习的训练过程通常都是一件非常耗时的事情，胡乱尝试时间成本是非常高的。\n",
    "\n",
    "- 这里的抽象成数学问题，指的是根据数据明确任务目标，是分类、还是回归，或者是聚类。\n",
    "\n",
    "### 2. 数据获取\n",
    "\n",
    "- 数据决定了机器学习结果的上限，而算法只是尽可能逼近这个上限。\n",
    "\n",
    "- 数据要有代表性，否则必然会过拟合。\n",
    "\n",
    "- 对于分类问题，数据偏斜不能过于严重（平衡），不同类别的数据数量不要有数个数量级的差距。\n",
    "\n",
    "- 对数据的量级要有一个评估，多少个样本，多少个特征，据此估算出内存需求。如果放不下就得考虑改进算法或者使用一些降维技巧，或者采用分布式计算。\n",
    "\n",
    "### 3. 预处理与特征选择\n",
    "\n",
    "- 良好的数据要能够提取出良好的特征才能真正发挥效力。\n",
    "\n",
    "- 预处理/数据清洗是很关键的步骤，往往能够使得算法的效果和性能得到显著提高。归一化、离散化、因子化、缺失值处理、去除共线性等，数据挖掘过程中很多时间就花在它们上面。这些工作简单可复制，收益稳定可预期，是机器学习的基础必备步骤。\n",
    "\n",
    "- 筛选出显著特征、摒弃非显著特征，需要机器学习工程师反复理解业务。这对很多结果有决定性的影响。特征选择好了，非常简单的算法也能得出良好、稳定的结果。这需要运用特征有效性分析的相关技术，如相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重等方法。\n",
    "\n",
    "### 4. 模型训练与调优\n",
    "\n",
    "- 直到这一步才用到我们上面说的算法进行训练。\n",
    "    \n",
    "- 现在很多算法都能够封装成黑盒使用。但是真正考验水平的是调整这些算法的（超）参数，使得结果变得更加优良。这需要我们对算法的原理有深入的理解。理解越深入，就越能发现问题的症结，提出良好的调优方案。\n",
    "\n",
    "### 5. 模型诊断\n",
    "\n",
    "- 如何确定模型调优的方向与思路呢？这就需要对模型进行诊断的技术。\n",
    "\n",
    "- 过拟合、欠拟合 判断是模型诊断中至关重要的一步。常见的方法如交叉验证，绘制学习曲线等。过拟合的基本调优思路是增加数据量，降低模型复杂度。欠拟合的基本调优思路是提高特征数量和质量，增加模型复杂度。\n",
    "\n",
    "- 误差分析也是机器学习至关重要的步骤。通过观察误差样本，全面分析误差产生误差的原因:是参数的问题还是算法选择的问题，是特征的问题还是数据本身的问题......\n",
    "\n",
    "- 诊断后的模型需要进行调优，调优后的新模型需要重新进行诊断，这是一个反复迭代不断逼近的过程，需要不断地尝试， 进而达到最优状态。\n",
    "\n",
    "### 6. 模型融合/集成\n",
    "\n",
    "- 一般来说，模型融合后都能使得效果有一定提升。而且效果很好。\n",
    "\n",
    "- 工程上，主要提升算法准确度的方法是分别在模型的前端（特征清洗和预处理，不同的采样模式）与后端（模型融合）上下功夫。因为他们比较标准可复制，效果比较稳定。而直接调参的工作不会很多，毕竟大量数据训练起来太慢了，而且效果难以保证。\n",
    "\n",
    "### 7. 上线运行\n",
    "\n",
    "- 这一部分内容主要跟工程实现的相关性更大。工程上是结果导向，模型在线上运行的效果直接决定模型的成败。不单纯包括其准确程度、误差等情况，还包括其运行的速度(时间复杂度)、资源消耗程度（空间复杂度）、稳定性是否可接受。\n",
    "\n",
    "- 这些工作流程主要是工程实践上总结出的一些经验。并不是每个项目都包含完整的一个流程。这里的部分只是一个指导性的说明，只有多实践，多积累项目经验，才会有自己更深刻的认识。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. 数据清洗与特征处理\n",
    "> geekcircle/machine-learning-interview-qa/[8.md](https://github.com/geekcircle/machine-learning-interview-qa/blob/master/questions/8.md)\n",
    "\n",
    "<div align=\"center\"><img src=\"source/数据清洗与特征处理.jpg\" height=\"\" /></div> \n",
    "\n",
    "> [机器学习中的数据清洗与特征处理综述](https://tech.meituan.com/machinelearning_data_feature_process.html) - 美团点评技术"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. 关联规则挖掘的 3 个度量指标：支持度、置信度、提升度\n",
    "\n",
    "### 支持度（Support）\n",
    "\n",
    "$X → Y$ 的支持度表示项集 ${X,Y}$ 在总项集中出现的概率\n",
    "\n",
    "<div align=\"center\"><img src=\"http://latex.codecogs.com/gif.latex?Support(X \\rightarrow  Y) = \\frac{P(X \\cup Y)}{P(I)} = \\frac{\\text{num}(X \\cup Y)}{\\text{num}(I)}\" /></div>\n",
    "\n",
    "其中，I 表示总事务集，`num()`表示事务集中特定项集出现的次数，`P(X)=num(X)/num(I)`\n",
    "\n",
    "### 置信度（Confidence）\n",
    "\n",
    "$X → Y$ 的置信度表示在先决条件 $X$ 发生的情况下，由规则 $X → Y$ 推出 $Y$ 的概率。\n",
    "\n",
    "<div align=\"center\"><img src=\"http://latex.codecogs.com/gif.latex?Confidence(X \\rightarrow Y) = P(Y|X) = \\frac{P(X \\cup Y)}{P(X)} = \\frac{\\text{num}(X \\cup Y)}{\\text{num}(X)}\" /></div>\n",
    "\n",
    "### 提升度（Lift）\n",
    "\n",
    "$X → Y$ 的提升度表示含有X的条件下，同时含有 $Y$ 的概率，与 $Y$ 总体发生的概率之比。\n",
    "\n",
    "<div align=\"center\"><img src=\"http://latex.codecogs.com/gif.latex?{\\displaystyle {\\begin{aligned} Lift(X \\rightarrow Y) &= \\frac{P(Y|X)}{P(Y)} = \\frac{Confidence(X \\rightarrow Y)}{\\text{num}(Y) / \\text{num}(I)} \\\\  &= \\frac{P(X \\cup Y)}{P(X)P(Y)} = \\frac{\\text{num}(X \\cup Y) \\text{num}(I)}{\\text{num}(X) \\text{num}(Y)} \\end{aligned}}}\" /></div>\n",
    "\n",
    "## 规则的有效性：\n",
    "\n",
    "- 满足最小支持度和最小置信度的规则，叫做“强关联规则”\n",
    "    > 最小支持度和最小置信度是人工设置的阈值\n",
    "- `Lift(X→Y) > 1` 的 X→Y 是有效的强关联规则\n",
    "- `Lift(X→Y) <=1` 的 X→Y 是有效的强关联规则\n",
    "- 特别地，`Lift(X→Y) = 1` 时，X 与 Y 相互独立。\n",
    "\n",
    "## 判断规则的有效性\n",
    "\n",
    "问题：已知有1000名顾客买年货，分为甲乙两组，每组各500人，其中甲组有500人买了茶叶，同时又有450人买了咖啡；乙组有450人买了咖啡，如表所示，请问“茶叶→咖啡”是一条有效的关联规则吗？\n",
    "\n",
    "组次 | 买茶叶的人数 | 买咖啡的人数\n",
    "--- | ---------- | ---------\n",
    " 甲组（500人） | 500 | 450\n",
    " 乙组（500人） | 0 | 450\n",
    "\n",
    "答：\n",
    "\n",
    "- “茶叶→咖啡”的支持度：Support(X→Y) = 450 / 1000 = 45%\n",
    "- “茶叶→咖啡”的置信度：Confidence(X→Y) = 450 / 500 = 90%\n",
    "- “茶叶→咖啡”的提升度：Lift(X→Y) = 90% / 90% = 1\n",
    " \n",
    "由于提升度 `Lift(X→Y) = 1`，表示 X 与 Y 相互独立。也就是说，是否购买咖啡，与是否购买茶叶无关联。规则“茶叶→咖啡”不成立，或者说几乎没有关联，虽然它的置信度高达90%，但它不是一条有效的关联规则。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 《神经网络与深度学习》3小时课程概要\n",
    "\n",
    "- [神经网络与深度学习](https://nndl.github.io/) - book\n",
    "\n",
    "- [微积分总结微积分总结 Summary of Calculus](https://hujiaweibujidao.github.io/files/calculus_summary.pdf) - pdf\n",
    "\n",
    "- [#机器学习数学基础# 可导，可微，导数，偏导数...都是些啥？](https://cloud.tencent.com/developer/article/1061415)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
